{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f22f6ccb-4b3c-4b30-a0d2-5ec48fb81cf6",
   "metadata": {},
   "source": [
    "## 10. (coding) Discretization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1404a6-d124-479a-9133-6ac8d7956bde",
   "metadata": {},
   "source": [
    "#### 1) Implement equal width discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c5b1d95-1fdb-444a-8969-842d771eb359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:  [1.5 2.3 4.7 3.6 5.8 7.9 8.2 9.4]\n",
      "Computed Bins:  [1.5        4.13333333 6.76666667 9.4       ]\n",
      "Discretized Data:  [0 1 2 1 2 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def equal_width(data, num_bins):\n",
    "    min_val = np.min(data)\n",
    "    max_val = np.max(data)\n",
    "    \n",
    "    bins = np.linspace(min_val, max_val, num_bins + 1)\n",
    "    \n",
    "    return bins\n",
    "\n",
    "# Continuous data\n",
    "data = np.array([1.5, 2.3, 4.7, 3.6, 5.8, 7.9, 8.2, 9.4])\n",
    "\n",
    "# Define the number of bins\n",
    "num_bins = 3\n",
    "\n",
    "# Compute intervals using the equal width function\n",
    "bins = equal_width(data, num_bins)\n",
    "\n",
    "# Using bins, discretize the data by assigning it to bins\n",
    "disc_data = np.digitize(data, bins, right=True)\n",
    "\n",
    "# Display the results\n",
    "print(\"Original Data: \", data)\n",
    "print(\"Computed Bins: \", bins)\n",
    "print(\"Discretized Data: \", disc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6961af21-3546-469b-9a2c-3caeb0434055",
   "metadata": {},
   "source": [
    "#### 2) Entropy-based discretization: The following program finds the best split point in a numeric data array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ff6c48-31ef-4561-ac0b-d8e3e229f2ac",
   "metadata": {},
   "source": [
    "#### Calculate Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1f00bf9-afb6-49e3-bc0b-78c01b7ab396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(y):\n",
    "\n",
    "    class_labels, counts = np.unique(y, return_counts=True)\n",
    "    probabilities = counts / len(y)\n",
    "\n",
    "    entropy = -np.sum([p * np.log2(p) for p in probabilities if p > 0])\n",
    "    \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39321c86-c147-4575-a8ce-07f72487b7fd",
   "metadata": {},
   "source": [
    "#### Calculate Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afb689e5-b372-4fd4-9749-03c89e840739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(y, y_left, y_right):\n",
    "    \n",
    "    entropy_before = calculate_entropy(y)\n",
    "\n",
    "    weight_left = len(y_left) / len(y)\n",
    "    weight_right = len(y_right) / len(y)\n",
    "    entropy_after = (weight_left * calculate_entropy(y_left) + \n",
    "                     weight_right * calculate_entropy(y_right))\n",
    "    \n",
    "    info_gain = entropy_before - entropy_after\n",
    "    \n",
    "    return info_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d1c90d-f28e-4ec3-9d08-6daa5f31a437",
   "metadata": {},
   "source": [
    "#### Find the Best Split Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5899534-5cf8-40dd-a996-9c42fde502be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_split(X, y):\n",
    "    best_info_gain = -1\n",
    "    best_split_point = None\n",
    "\n",
    "    for value in np.unique(X):\n",
    "        y_left = y[X <= value]\n",
    "        y_right = y[X > value]\n",
    "        \n",
    "        current_info_gain = information_gain(y, y_left, y_right)\n",
    "        \n",
    "        if current_info_gain > best_info_gain:\n",
    "            best_info_gain = current_info_gain\n",
    "            best_split_point = value\n",
    "    \n",
    "    return best_split_point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f560b8-b258-46d8-8dda-d38e88d58437",
   "metadata": {},
   "source": [
    "#### Example Usage and Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8498883-d9ef-4325-80c6-c8294516d19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Split Point: 2\n"
     ]
    }
   ],
   "source": [
    "X = np.array([1, 2, 3, 4, 5, 6])\n",
    "y = np.array([0, 0, 1, 0, 1, 1])\n",
    "\n",
    "split_point = best_split(X, y)\n",
    "\n",
    "print(f\"Best Split Point: {split_point}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065ee262-4744-4c95-b708-57e1f5a307ca",
   "metadata": {},
   "source": [
    "## 11. (coding) Aprior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4582b9a8-4808-4e01-a1de-48ce880614b1",
   "metadata": {},
   "source": [
    "#### Transaction Database and the Minimum Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a17b37df-dcc4-4cd9-abd1-506dd2919762",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_db = [['milk','bread','biscuit'], ['bread','milk','biscuit','cornflakes'],\n",
    "            ['bread','tea','bournvita'], ['jam','maggi','bread','milk'], ['maggi','tea','biscuit'],\n",
    "            ['bread','tea','bournvita'], ['maggi','tea','cornflakes'], ['maggi','bread','tea','biscuit'],\n",
    "            ['jam','maggi','bread','tea'], ['bread','milk'], ['coffee','cock','biscuit','cornflakes'],\n",
    "            ['coffee','cock','biscuit','cornflakes'], ['coffee','sugar','bournvita'], ['bread','coffee','cock'],\n",
    "            ['bread','sugar','biscuit'], ['coffee','sugar','cornflakes'], ['bread','sugar','bournvita'],\n",
    "            ['bread','coffee','sugar'], ['bread','coffee','sugar'], ['tea','milk','coffee','cornflakes']]\n",
    "\n",
    "min_support = 3\n",
    "infreq_itemsets = []\n",
    "\n",
    "# Compute support for an itemset\n",
    "def compute_support(itemset):\n",
    "    support = 0\n",
    "    for trans in trans_db:\n",
    "        if set(itemset).issubset(set(trans)):\n",
    "            support += 1\n",
    "    return support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891f93f0-416b-4c23-9480-623178566269",
   "metadata": {},
   "source": [
    "#### Generate k+1 Frequent Itemsets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b44b019f-f1e5-4a01-aa28-b60381feee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate (k+1) itemsets from k itemsets\n",
    "def generate_k_1_itemsets(k_itemsets):\n",
    "    k_1_itemsets = []\n",
    "    for i in range(len(k_itemsets)):\n",
    "        for j in range(i + 1, len(k_itemsets)):\n",
    "            # Create new itemset by combining the two k-itemsets\n",
    "            new_itemset = list(set(k_itemsets[i]) | set(k_itemsets[j]))\n",
    "            # Ensure new itemset has length k+1 and is not infrequent\n",
    "            if len(new_itemset) == len(k_itemsets[0]) + 1 and new_itemset not in infreq_itemsets:\n",
    "                # Check support of the new itemset\n",
    "                if compute_support(new_itemset) >= min_support:\n",
    "                    k_1_itemsets.append(new_itemset)\n",
    "                else:\n",
    "                    infreq_itemsets.append(new_itemset)  # Mark as infrequent if support < min_support\n",
    "    return k_1_itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cf905f-1d12-4e29-af73-8027f03fb84c",
   "metadata": {},
   "source": [
    "#### Generate Frequent 1-itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cfe4ecd9-780c-4e49-adb9-19f565a134ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent 1-itemsets: [['sugar'], ['bournvita'], ['tea'], ['cock'], ['bread'], ['maggi'], ['milk'], ['biscuit'], ['coffee'], ['cornflakes']]\n"
     ]
    }
   ],
   "source": [
    "all_list_items = list(set(i for j in trans_db for i in j))\n",
    "itemsets_1 = []\n",
    "\n",
    "for item in all_list_items:\n",
    "    if compute_support([item]) >= min_support:\n",
    "        itemsets_1.append([item])\n",
    "\n",
    "print(\"Frequent 1-itemsets:\", itemsets_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2bfa3b-f1ca-40b8-a464-29ec3bc68bdc",
   "metadata": {},
   "source": [
    "#### Generate Frequent 2-itemsets and 3-itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa4c16a9-89e6-4583-b690-1de72091a703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent 2-itemsets: [['sugar', 'bread'], ['sugar', 'coffee'], ['bournvita', 'bread'], ['tea', 'bread'], ['maggi', 'tea'], ['coffee', 'cock'], ['maggi', 'bread'], ['milk', 'bread'], ['bread', 'biscuit'], ['coffee', 'bread'], ['cornflakes', 'biscuit'], ['coffee', 'cornflakes']]\n",
      "Frequent 3-itemsets: []\n"
     ]
    }
   ],
   "source": [
    "itemsets_2 = generate_k_1_itemsets(itemsets_1)\n",
    "print(\"Frequent 2-itemsets:\", itemsets_2)\n",
    "\n",
    "itemsets_3 = generate_k_1_itemsets(itemsets_2)\n",
    "print(\"Frequent 3-itemsets:\", itemsets_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a03921-9029-405f-8cea-8af913c56570",
   "metadata": {},
   "source": [
    "## 12. Discretization using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04cc1262-d5db-4345-adb2-2def7518e15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "    sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0                5.1               3.5                1.4               0.2\n",
      "1                4.9               3.0                1.4               0.2\n",
      "2                4.7               3.2                1.3               0.2\n",
      "3                4.6               3.1                1.5               0.2\n",
      "4                5.0               3.6                1.4               0.2 \n",
      "\n",
      "Equal-width discretized Data:\n",
      "    sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0                0.0               1.0                0.0               0.0\n",
      "1                0.0               1.0                0.0               0.0\n",
      "2                0.0               1.0                0.0               0.0\n",
      "3                0.0               1.0                0.0               0.0\n",
      "4                0.0               1.0                0.0               0.0 \n",
      "\n",
      "Equal-frequency discretized Data:\n",
      "    sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0                0.0               2.0                0.0               0.0\n",
      "1                0.0               1.0                0.0               0.0\n",
      "2                0.0               2.0                0.0               0.0\n",
      "3                0.0               1.0                0.0               0.0\n",
      "4                0.0               2.0                0.0               0.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_iris()\n",
    "X = dataset.data \n",
    "y = dataset.target \n",
    "feature_names = dataset.feature_names\n",
    "\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "\n",
    "equal_width_discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n",
    "X_equal_width = equal_width_discretizer.fit_transform(X)\n",
    "\n",
    "equal_freq_discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='quantile')\n",
    "X_equal_freq = equal_freq_discretizer.fit_transform(X)\n",
    "\n",
    "df_equal_width = pd.DataFrame(X_equal_width, columns=feature_names)\n",
    "df_equal_freq = pd.DataFrame(X_equal_freq, columns=feature_names)\n",
    "\n",
    "print(\"Original Data:\\n\", df.head(), \"\\n\")\n",
    "print(\"Equal-width discretized Data:\\n\", df_equal_width.head(), \"\\n\")\n",
    "print(\"Equal-frequency discretized Data:\\n\", df_equal_freq.head(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7479028f-de45-4ee0-8c33-13d134a7510c",
   "metadata": {},
   "source": [
    "## 13. Apriori using library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e328d9c8-4b1b-490b-b227-2cc93d3b5a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Itemsets:\n",
      "    support               itemsets\n",
      "0      0.35              (biscuit)\n",
      "1      0.20            (bournvita)\n",
      "2      0.65                (bread)\n",
      "3      0.15                 (cock)\n",
      "4      0.40               (coffee)\n",
      "5      0.30           (cornflakes)\n",
      "6      0.25                (maggi)\n",
      "7      0.25                 (milk)\n",
      "8      0.30                (sugar)\n",
      "9      0.35                  (tea)\n",
      "10     0.20       (bread, biscuit)\n",
      "11     0.15  (cornflakes, biscuit)\n",
      "12     0.15     (bournvita, bread)\n",
      "13     0.15        (coffee, bread)\n",
      "14     0.15         (maggi, bread)\n",
      "15     0.20          (milk, bread)\n",
      "16     0.20         (sugar, bread)\n",
      "17     0.20           (tea, bread)\n",
      "18     0.15         (coffee, cock)\n",
      "19     0.20   (coffee, cornflakes)\n",
      "20     0.20        (sugar, coffee)\n",
      "21     0.20           (maggi, tea)\n",
      "\n",
      "Association Rules:\n",
      "   antecedents consequents  antecedent support  consequent support  support  \\\n",
      "0  (bournvita)     (bread)                0.20                0.65     0.15   \n",
      "1       (milk)     (bread)                0.25                0.65     0.20   \n",
      "2       (cock)    (coffee)                0.15                0.40     0.15   \n",
      "3      (maggi)       (tea)                0.25                0.35     0.20   \n",
      "\n",
      "   confidence      lift  leverage  conviction  zhangs_metric  \n",
      "0        0.75  1.153846    0.0200        1.40       0.166667  \n",
      "1        0.80  1.230769    0.0375        1.75       0.250000  \n",
      "2        1.00  2.500000    0.0900         inf       0.705882  \n",
      "3        0.80  2.285714    0.1125        3.25       0.750000  \n"
     ]
    }
   ],
   "source": [
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "trans_db = [\n",
    "    ['milk', 'bread', 'biscuit'],\n",
    "    ['bread', 'milk', 'biscuit', 'cornflakes'],\n",
    "    ['bread', 'tea', 'bournvita'],\n",
    "    ['jam', 'maggi', 'bread', 'milk'],\n",
    "    ['maggi', 'tea', 'biscuit'],\n",
    "    ['bread', 'tea', 'bournvita'],\n",
    "    ['maggi', 'tea', 'cornflakes'],\n",
    "    ['maggi', 'bread', 'tea', 'biscuit'],\n",
    "    ['jam', 'maggi', 'bread', 'tea'],\n",
    "    ['bread', 'milk'],\n",
    "    ['coffee', 'cock', 'biscuit', 'cornflakes'],\n",
    "    ['coffee', 'cock', 'biscuit', 'cornflakes'],\n",
    "    ['coffee', 'sugar', 'bournvita'],\n",
    "    ['bread', 'coffee', 'cock'],\n",
    "    ['bread', 'sugar', 'biscuit'],\n",
    "    ['coffee', 'sugar', 'cornflakes'],\n",
    "    ['bread', 'sugar', 'bournvita'],\n",
    "    ['bread', 'coffee', 'sugar'],\n",
    "    ['bread', 'coffee', 'sugar'],\n",
    "    ['tea', 'milk', 'coffee', 'cornflakes']\n",
    "]\n",
    "\n",
    "te = TransactionEncoder()\n",
    "trans_array = te.fit(trans_db).transform(trans_db)\n",
    "trans_df = pd.DataFrame(trans_array, columns=te.columns_)\n",
    "\n",
    "min_support = 3 / len(trans_db)  \n",
    "frequent_itemsets = apriori(trans_df, min_support=min_support, use_colnames=True)\n",
    "\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
    "\n",
    "print(\"Frequent Itemsets:\")\n",
    "print(frequent_itemsets)\n",
    "\n",
    "print(\"\\nAssociation Rules:\")\n",
    "print(rules)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
