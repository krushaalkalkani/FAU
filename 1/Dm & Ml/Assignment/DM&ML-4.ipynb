{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import datasets, layers, models\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.applications import VGG16\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "\n",
    "# Creating a list of all the class labels\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Converting the pixels data to float type\n",
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    "\n",
    "# Normalize input to range [0, 1]\n",
    "train_images = train_images / 255\n",
    "test_images = test_images / 255\n",
    "\n",
    "# Resize images to (224, 224) for VGG16\n",
    "train_images_resized = tf.image.resize(train_images, (224, 224)).numpy()\n",
    "test_images_resized = tf.image.resize(test_images, (224, 224)).numpy()\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "train_labels = to_categorical(train_labels, num_classes=10)\n",
    "test_labels = to_categorical(test_labels, num_classes=10)\n",
    "\n",
    "# Define the inputs and outputs\n",
    "X_train = train_images_resized\n",
    "X_test = test_images_resized\n",
    "Y_train = train_labels\n",
    "Y_test = test_labels\n",
    "\n",
    "# --- Part 14.1-2 (2 pts) --- Show the shape of datasets and one sample image ---\n",
    "print(f\"Shape of train images: {train_images.shape}, Shape of train labels: {train_labels.shape}\")\n",
    "print(f\"Shape of test images: {test_images.shape}, Shape of test labels: {test_labels.shape}\")\n",
    "\n",
    "# Show one sample image\n",
    "plt.imshow(X_train[0])\n",
    "plt.title(f\"Sample image: {class_names[np.argmax(Y_train[0])]}\")\n",
    "plt.show()\n",
    "\n",
    "# --- Part 14.1-3 (4 pts) --- Create the model using transfer learning ---\n",
    "def create_model(input_shape, fine_tune=0):\n",
    "    # Load VGG16 pre-trained model without the top fully connected layers\n",
    "    conv_base = VGG16(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    \n",
    "    # Freeze layers based on fine_tune parameter\n",
    "    if fine_tune > 0:\n",
    "        for layer in conv_base.layers[:-fine_tune]:\n",
    "            layer.trainable = False\n",
    "    else:\n",
    "        for layer in conv_base.layers:\n",
    "            layer.trainable = False\n",
    "    \n",
    "    # Create a new model on top of the frozen VGG16 base\n",
    "    model = models.Sequential([\n",
    "        conv_base,\n",
    "        Flatten(),\n",
    "        Dense(20, activation='relu'),  # 1st dense layer with 20 units\n",
    "        Dense(10, activation='relu'),  # 2nd dense layer with 10 units\n",
    "        Dense(10, activation='softmax')  # Output layer with 10 units (for 10 classes)\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model with no fine-tuning initially\n",
    "input_shape = X_train[0].shape\n",
    "model = create_model(input_shape, fine_tune=0)\n",
    "\n",
    "# Display the model architecture\n",
    "print(model.summary())\n",
    "\n",
    "# --- Part 14.1-4 (4 pts) --- Compile, train the model and evaluate ---\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, Y_train, epochs=10, batch_size=64, validation_data=(X_test, Y_test))\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test, verbose=2)\n",
    "print(f\"Test accuracy: {test_acc}\")\n",
    "\n",
    "# --- Part 14.2 --- Plot accuracy vs number of trainable layers ---\n",
    "def plot_accuracy_vs_layers():\n",
    "    fine_tune_range = range(0, 16)  # 0 to 15 layers of VGG16 can be fine-tuned\n",
    "    accuracies = []\n",
    "\n",
    "    for fine_tune in fine_tune_range:\n",
    "        model = create_model(input_shape, fine_tune=fine_tune)\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        model.fit(X_train, Y_train, epochs=5, batch_size=64, verbose=0)\n",
    "        test_loss, test_acc = model.evaluate(X_test, Y_test, verbose=0)\n",
    "        accuracies.append(test_acc)\n",
    "\n",
    "    # Plotting the accuracy\n",
    "    plt.plot(fine_tune_range, accuracies)\n",
    "    plt.xlabel(\"Number of Trainable Layers\")\n",
    "    plt.ylabel(\"Test Accuracy\")\n",
    "    plt.title(\"Accuracy vs Number of Trainable Layers\")\n",
    "    plt.show()\n",
    "\n",
    "# Plot the accuracy vs the number of trainable layers\n",
    "plot_accuracy_vs_layers()\n",
    "\n",
    "# --- Part 14.3 Bonus --- Use MNIST dataset as input and repeat the above process ---\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# Load MNIST dataset\n",
    "(train_images_mnist, train_labels_mnist), (test_images_mnist, test_labels_mnist) = mnist.load_data()\n",
    "\n",
    "# Preprocess MNIST data\n",
    "train_images_mnist = np.expand_dims(train_images_mnist, axis=-1).astype('float32')\n",
    "test_images_mnist = np.expand_dims(test_images_mnist, axis=-1).astype('float32')\n",
    "train_images_mnist /= 255\n",
    "test_images_mnist /= 255\n",
    "\n",
    "# Resize MNIST images to (224, 224, 3) for VGG16\n",
    "train_images_mnist_resized = tf.image.resize(train_images_mnist, (224, 224)).numpy()\n",
    "test_images_mnist_resized = tf.image.resize(test_images_mnist, (224, 224)).numpy()\n",
    "\n",
    "# Convert labels to one-hot encoding for MNIST\n",
    "train_labels_mnist = to_categorical(train_labels_mnist, num_classes=10)\n",
    "test_labels_mnist = to_categorical(test_labels_mnist, num_classes=10)\n",
    "\n",
    "# Define the inputs and outputs for MNIST\n",
    "X_train_mnist = train_images_mnist_resized\n",
    "X_test_mnist = test_images_mnist_resized\n",
    "Y_train_mnist = train_labels_mnist\n",
    "Y_test_mnist = test_labels_mnist\n",
    "\n",
    "# Create the model with no fine-tuning initially for MNIST\n",
    "input_shape_mnist = X_train_mnist[0].shape\n",
    "model_mnist = create_model(input_shape_mnist, fine_tune=0)\n",
    "\n",
    "# Train and evaluate the model on MNIST\n",
    "model_mnist.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history_mnist = model_mnist.fit(X_train_mnist, Y_train_mnist, epochs=10, batch_size=64, validation_data=(X_test_mnist, Y_test_mnist))\n",
    "test_loss_mnist, test_acc_mnist = model_mnist.evaluate(X_test_mnist, Y_test_mnist, verbose=2)\n",
    "print(f\"Test accuracy on MNIST: {test_acc_mnist}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the function and its gradients\n",
    "def f(x, y):\n",
    "    return x**2 + 2*y + 3*x\n",
    "\n",
    "# Compute the derivative with respect to x\n",
    "def f_der_x(x):\n",
    "    ''' Derivative of f(x, y) with respect to x '''\n",
    "    return 2*x + 3\n",
    "\n",
    "# Compute the derivative with respect to y\n",
    "def f_der_y(y):\n",
    "    ''' Derivative of f(x, y) with respect to y '''\n",
    "    return 2\n",
    "\n",
    "# Implement RMSProp\n",
    "# RMSProp parameters\n",
    "gamma = 0.8  # Momentum factor\n",
    "lrate = 0.01  # Learning rate\n",
    "epsilon = 1e-8  # Small constant to avoid division by zero\n",
    "\n",
    "# Accumulated gradient squares for x and y, respectively\n",
    "gt_x = 0\n",
    "gt_y = 0\n",
    "\n",
    "# Initial values of x and y\n",
    "x, y = 10.0, -10.0\n",
    "\n",
    "# Number of iterations\n",
    "no_iterations = 100\n",
    "\n",
    "# RMSProp optimization loop\n",
    "for i in range(no_iterations):\n",
    "    # Compute the gradients for x and y\n",
    "    grad_x = f_der_x(x)\n",
    "    grad_y = f_der_y(y)\n",
    "    \n",
    "    # Update accumulated gradient squares for x and y\n",
    "    gt_x = gamma * gt_x + (1 - gamma) * grad_x**2\n",
    "    gt_y = gamma * gt_y + (1 - gamma) * grad_y**2\n",
    "    \n",
    "    # Update variables x and y\n",
    "    x -= (lrate / (np.sqrt(gt_x) + epsilon)) * grad_x\n",
    "    y -= (lrate / (np.sqrt(gt_y) + epsilon)) * grad_y\n",
    "    \n",
    "    # Show the progress\n",
    "    print(f\"Iteration = {i + 1}: x = {x}, y = {y}, f(x, y) = {f(x, y)}\")\n",
    "\n",
    "# Show the final x value and f value\n",
    "print(f\"Final: x = {x}, y = {y}, f(x, y) = {f(x, y)}\")\n",
    "\n",
    "# Bonus 1: L2 regularization\n",
    "def f_with_l2(x, y, lambda_reg=0.1):\n",
    "    '''Function with L2 regularization'''\n",
    "    return f(x, y) + lambda_reg * (x**2 + y**2)\n",
    "\n",
    "def f_der_x_l2(x, lambda_reg=0.1):\n",
    "    '''Derivative of f(x, y) with respect to x with L2 regularization'''\n",
    "    return f_der_x(x) + 2 * lambda_reg * x\n",
    "\n",
    "def f_der_y_l2(y, lambda_reg=0.1):\n",
    "    '''Derivative of f(x, y) with respect to y with L2 regularization'''\n",
    "    return f_der_y(y) + 2 * lambda_reg * y\n",
    "\n",
    "# Implement RMSProp with L2 regularization\n",
    "x, y = 10.0, -10.0\n",
    "gt_x, gt_y = 0, 0\n",
    "lambda_reg = 0.1  # Regularization strength\n",
    "\n",
    "for i in range(no_iterations):\n",
    "    grad_x = f_der_x_l2(x, lambda_reg)\n",
    "    grad_y = f_der_y_l2(y, lambda_reg)\n",
    "    \n",
    "    gt_x = gamma * gt_x + (1 - gamma) * grad_x**2\n",
    "    gt_y = gamma * gt_y + (1 - gamma) * grad_y**2\n",
    "    \n",
    "    x -= (lrate / (np.sqrt(gt_x) + epsilon)) * grad_x\n",
    "    y -= (lrate / (np.sqrt(gt_y) + epsilon)) * grad_y\n",
    "    \n",
    "    print(f\"Iteration = {i + 1}: x = {x}, y = {y}, f(x, y) = {f_with_l2(x, y, lambda_reg)}\")\n",
    "\n",
    "# Show the final x value and f value with L2 regularization\n",
    "print(f\"Final (with L2): x = {x}, y = {y}, f(x, y) = {f_with_l2(x, y, lambda_reg)}\")\n",
    "\n",
    "# Bonus 2: Implement AdaDelta\n",
    "def adadelta_optimizer(x, y, gamma=0.8, epsilon=1e-8, lrate=0.01, no_iterations=100):\n",
    "    '''AdaDelta optimizer with RMSProp-like behavior'''\n",
    "\n",
    "    # Accumulated squared gradients for x and y\n",
    "    gt_x, gt_y = 0, 0\n",
    "    # Accumulated squared updates for x and y\n",
    "    st_x, st_y = 0, 0\n",
    "\n",
    "    for i in range(no_iterations):\n",
    "        # Compute the gradients for x and y\n",
    "        grad_x = f_der_x(x)\n",
    "        grad_y = f_der_y(y)\n",
    "        \n",
    "        # Update accumulated gradient squares (gt) for x and y\n",
    "        gt_x = gamma * gt_x + (1 - gamma) * grad_x**2\n",
    "        gt_y = gamma * gt_y + (1 - gamma) * grad_y**2\n",
    "        \n",
    "        # Update accumulated squared updates (st) for x and y\n",
    "        st_x = gamma * st_x + (1 - gamma) * grad_x**2\n",
    "        st_y = gamma * st_y + (1 - gamma) * grad_y**2\n",
    "        \n",
    "        # Compute the update based on AdaDelta formula\n",
    "        delta_x = np.sqrt((st_x + epsilon) / (gt_x + epsilon)) * grad_x\n",
    "        delta_y = np.sqrt((st_y + epsilon) / (gt_y + epsilon)) * grad_y\n",
    "        \n",
    "        # Update the variables x and y\n",
    "        x -= lrate * delta_x\n",
    "        y -= lrate * delta_y\n",
    "        \n",
    "        # Show the progress\n",
    "        print(f\"Iteration = {i + 1}: x = {x}, y = {y}, f(x, y) = {f(x, y)}\")\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "# Run AdaDelta\n",
    "x, y = 10.0, -10.0\n",
    "x, y = adadelta_optimizer(x, y)\n",
    "\n",
    "# Show the final x value and f value after AdaDelta\n",
    "print(f\"Final (AdaDelta): x = {x}, y = {y}, f(x, y) = {f(x, y)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.6' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the true function\n",
    "# Suppose the true error function is cosine function\n",
    "def true_function(X):\n",
    "    return np.cos(X)\n",
    "\n",
    "# Define the RBF (Radial Basis Function) kernel (Gaussian Kernel)\n",
    "# Computes the RBF (Gaussian) kernel between two vectors X1 and X2.\n",
    "# X1 : numpy array 1D (size=n)\n",
    "# X2 : numpy array 1D (size=m)\n",
    "# lambda(𝜆) : Kernel coefficient for RBF\n",
    "# returns:\n",
    "# K : numpy array of shape (size of X1=n, size of X2=m).\n",
    "def rbf_kernel(X1, X2, lambda_val=1.0):\n",
    "    '''RBF kernel (Gaussian kernel) implementation'''\n",
    "    # Compute squared distances\n",
    "    sq_dist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * np.dot(X1, X2.T)\n",
    "    # Apply the Gaussian kernel formula\n",
    "    K = np.exp(-lambda_val * sq_dist)\n",
    "    return K\n",
    "\n",
    "# Define the Gaussian Process Regression function\n",
    "def gaussian_regression(X_train, y_train, X_test, lambda_val=1.0):\n",
    "    '''Perform Gaussian Process Regression'''\n",
    "    # Step 1: Compute the covariance matrix K(X_train, X_train)\n",
    "    K = rbf_kernel(X_train, X_train, lambda_val)  # Covariance of the training data\n",
    "    \n",
    "    # Step 2: Compute the covariance between X_train and X_test: k(X_train, X_test)\n",
    "    K_s = rbf_kernel(X_train, X_test, lambda_val)  # Covariance between X_train and X_test\n",
    "    \n",
    "    # Step 3: Compute the covariance matrix K(X_test, X_test)\n",
    "    K_ss = rbf_kernel(X_test, X_test, lambda_val)  # Covariance of the test data\n",
    "    \n",
    "    # Step 4: Compute the inverse of K(X_train, X_train)\n",
    "    K_inv = np.linalg.inv(K + 1e-8 * np.eye(len(X_train)))  # Regularize with small epsilon to avoid singular matrix\n",
    "    \n",
    "    # Step 5: Compute the mean of the posterior predictive distribution\n",
    "    mu_star = np.dot(K_s.T, np.dot(K_inv, y_train))\n",
    "    \n",
    "    # Step 6: Compute the covariance of the posterior predictive distribution\n",
    "    sigma_star = K_ss - np.dot(K_s.T, np.dot(K_inv, K_s))\n",
    "    \n",
    "    return mu_star, sigma_star\n",
    "\n",
    "# Suppose you did the following 6 experiments changing the values of ‘X_train’.\n",
    "# ‘y_train’ is its corresponding error function value\n",
    "X_train = np.array([[1], [3], [5], [6], [7], [8]])\n",
    "y_train = true_function(X_train).ravel()\n",
    "\n",
    "# Let’s estimate the error function value when x=2.2\n",
    "X_test = np.array([[2.2]])\n",
    "\n",
    "# 16. 1)-3: Estimate mean and covariance of ‘X_test’\n",
    "mu_star, sigma_star = gaussian_regression(X_train, y_train, X_test)\n",
    "\n",
    "# Compute standard deviation (diagonal values of covariance matrix)\n",
    "sd = np.sqrt(np.diagonal(sigma_star))\n",
    "\n",
    "print(f\"Mean for X_test=2.2: {mu_star}\")\n",
    "print(f\"Standard deviation for X_test=2.2: {sd}\")\n",
    "\n",
    "# 2) Change X_test to X_test=np.array([[3.4]]). Compute mean and s.d. of X_test=3.4.\n",
    "X_test = np.array([[3.4]])\n",
    "mu_star, sigma_star = gaussian_regression(X_train, y_train, X_test)\n",
    "sd = np.sqrt(np.diagonal(sigma_star))\n",
    "\n",
    "print(f\"Mean for X_test=3.4: {mu_star}\")\n",
    "print(f\"Standard deviation for X_test=3.4: {sd}\")\n",
    "\n",
    "# 3) Change X_test to a set of values (0 to 10 in 10 steps)\n",
    "X_test = np.linspace(0, 10, 10).reshape(-1, 1)\n",
    "\n",
    "# Compute mean and standard deviation for the new test points\n",
    "mu_star, sigma_star = gaussian_regression(X_train, y_train, X_test)\n",
    "sd = np.sqrt(np.diagonal(sigma_star))\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure()\n",
    "plt.plot(X_test, true_function(X_test), 'r:', label=\"True function\")\n",
    "plt.plot(X_train, y_train, 'r.', markersize=10, label=\"Training data\")\n",
    "plt.plot(X_test, mu_star, 'b-', label=\"Prediction\")\n",
    "plt.fill_between(X_test.ravel(), mu_star - 1.96*sd, mu_star + 1.96*sd, alpha=0.2, color='b', label=\"Confidence interval\")\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Gaussian Process Regression')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 5) Lower Confidence Bound (LCB)\n",
    "# For each value in X_test, compute LCB(x) = mu(x) - k * sigma(x)\n",
    "k = 1.96  # For 95% confidence interval, you can adjust 'k' as needed\n",
    "LCB = mu_star - k * sd\n",
    "\n",
    "print(f\"Lower Confidence Bound (LCB) for each X_test value:\")\n",
    "print(LCB)\n",
    "\n",
    "# You can choose the next X value based on LCB values (e.g., choose the X value with the minimum LCB for exploration)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
